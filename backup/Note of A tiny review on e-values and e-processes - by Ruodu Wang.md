# Def (e-variable)
E-variable for a hypothesis $H$ is a $[0, \infty]$-valued random variable satisfied 

$$E^{Q}(E) \leq 1,$$

where $Q$ is the only one probability measure to represent $H$.

**Remark 1**: This definition do not give a unnique method to conduct a e-variable, which means it is open to methods that can satisfy it. This point is vert crucial for us to understand how to test hypothesis by e-variable.



# Basic examples and properties
For the simplest example, suppose that we are testing a simple hypothesis $Q_0$ versus a simple hypothesis $Q_1$, where $Q_1$ is absolutely continuous with respect to $Q_0.$ For this setting, a natural e-variable is the likelihood ratio $E$ = d$Q_1/$d$Q_0(X)$ where $X$ is the observed data. It is straightforward to verify that $E\geq0$ and it satisfes $\mathbb{E}^Q_0[E]=1.$ If we observe iid data $X_1,X_2,\ldots$ sequentially, then the likelihood ratio process $M$ given by

$$M_0=1\quad\mathrm{~and~}\quad M_t=\prod_{k=1}^t\frac{\mathrm{d}Q_1}{\mathrm{d}Q_0}(X_k)\text{ for }t=1,2,\ldots $$

is an e-process adapted to the filtration generated by the data. Moreover, we can easily see that $M$ is a martingale. Indeed, when testing simple hypotheses, it is optimal in a natural sense to use a martingale to construct e-processes. For composite hypotheses, the situation is much more complicated, as non-trivial composite martingales may not exist while non-trivial e-processes may exist (Ramdas et al. (2020)).

Let $E$ be an e-variable tor $H$. An important property ot e-variables is the inequality $Q(E\geq1/\alpha)\leq\alpha$ for any $\alpha\in(0,1)$ and $Q\in H$, due to Markov's inequality. Moreover, for any non-negative supermartingale $M$ under $Q$ with $M(0)=1$, Ville $(1939)â€™$s inequality gives

$$Q\left(\sup_{t=0,1,...,T}M_t\geq\frac1\alpha\right)\leq\alpha,\quad\alpha\in(0,1);$$

here $T$ may be finite or infnite. Moreover, any e-process for $H$ is dominated by a class of supermartingales $M^Q$ with initial value 1 for $Q\in H$, all with respect to the same filtration (Ramdas et al. (2020)). This insight implies that tests formulated by rejecting the null hypothesis if an e-process goes above $1/\alpha$ are anytime-$valid;$ that is, its type-I error is controlled at $\alpha$ regardless of the stopping rule.

**Remark 2:** The supermartingales $M^Q$ does not mean it is the sequence of likelihood ratio, of course. Just we disscuss in **Remark 1** which the definition of e-variable is open to methods that can satisfy it. Therefore if every element of one supermartingale all can satisfy the definition of e-variable, we can say this supermartingales is a e-process which can be used to do hypothesis testing.



# Calibrators
Calibrators, under various names, are studied by Shafer et al. (2011), Shafer (2021) and Vovk and Wang (2021). A decreasing function $f:[0,1]\to[0,\infty]$ is an admissible p-to-e calibrator if and only if $f$ is upper semicontinuous, $f(0)=\infty$, and $\int_0^1f=1.$ Simple examples of p-to-e calibrators are $f(p)=\kappa p^{\kappa-1}$ for some $\kappa\in(0,1)$ and $f(p)=p^{-1/2}-1$ (Shafer's). On the other hand, the only admissible e-to-p calibrator is given by $f:[0,\infty]\to[0,1],f(e)=\min(1/e,1);$ this is again due to Markov's inequality. Hence, for any e-variable $E,1/E$ truncated at 1 is a p-variable. If further $E$ has a decreasing density on $(0,\infty)$, then $1/(2E)$ is a p-variable (Wang (2023)). Converting a p-value to an e-value using a p-to-e calibrator and then back to p-value using an e-to-p calibrator generally loses quite a lot of evidence. For instance starting with $p=0.01$, a conversion with the p-to-e calibrator $p\mapsto p^{-1/2}-1$ gives $e=9$, and another conversion with the e-to-p calibrator $e\mapsto\min(1/e,1)$ yields $p^\prime=1/9.$